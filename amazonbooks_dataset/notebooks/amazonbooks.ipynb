{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Amazon Books – Data Preparation Pipeline\n",
    "\n",
    "This notebook builds a complete, reproducible preprocessing pipeline for the Amazon Books dataset.\n",
    "Starting from the raw Kaggle export, it produces:\n",
    "\n",
    "1. A cleaned and deduplicated user–item interaction table (`ratings.csv`) with normalized timestamps and one canonical `itemId` per book title.\n",
    "2. A time-aware leave-one-out (LOO) split with train, validation, and test targets, plus aligned user/item index maps.\n",
    "3. Labeled interaction logs for training recommendation baselines and LLM prompts, including positive/negative labels.\n",
    "4. Item–item similarity candidates and 100-user candidate pools with ground-truth targets injected.\n",
    "5. Truncated user histories with at most 5 recent positives per user to control sequence length.\n",
    "6. Final CSV exports of splits, candidate pools, and user/item maps under `csv_export/`.\n",
    "7. Lightweight item-level metadata (title + one review field) and a richer per-item description table built from multiple reviews.\n",
    "8. Consistency and quality checks on both interactions and metadata to ensure that all downstream components see a clean, one-to-one mapping between IDs and titles."
   ],
   "id": "3fde3b9f7186896c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Reset local output folders\n",
    "\n",
    "I start by clearing any previous outputs under `splits/`, `candidates_subset100/`, and `csv_export/`.\n",
    "This ensures the notebook can be rerun from scratch without leftover Parquet or CSV files that might be inconsistent with the current code."
   ],
   "id": "d2604e1cb01ff854"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-06T17:58:19.067338Z",
     "start_time": "2025-12-06T17:58:19.061972Z"
    }
   },
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\")\n",
    "\n",
    "folders_to_clean = [\n",
    "    \"splits\",\n",
    "    \"candidates_subset100\",\n",
    "    \"csv_export\"\n",
    "]\n",
    "\n",
    "for d in folders_to_clean:\n",
    "    p = BASE / d\n",
    "    if p.exists():\n",
    "        shutil.rmtree(p)\n",
    "        print(\"[clean] removed\", p)\n",
    "    else:\n",
    "        print(\"[clean] not found\", p)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean] not found C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\splits\n",
      "[clean] not found C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\candidates_subset100\n",
      "[clean] not found C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\csv_export\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load and normalize raw Amazon ratings\n",
    "\n",
    "The raw `Books_rating.csv` file from Kaggle is loaded and converted into a compact interaction table with four columns: `userId`, `itemId`, `rating`, and `timestamp`.\n",
    "Timestamps are parsed into a consistent Unix second format, non-numeric ratings are coerced to floats, and only ratings in the closed interval `[1, 5]` are kept.\n",
    "The result is saved as `ratings.csv`, which serves as the starting point for all subsequent filtering and deduplication."
   ],
   "id": "a62d9972e3a1974f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:58:56.448323Z",
     "start_time": "2025-12-06T17:58:21.655729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CSV_PATH = BASE / \"Books_rating.csv\"\n",
    "assert CSV_PATH.exists(), f\"Missing file: {CSV_PATH}\"\n",
    "\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "print(\"Raw columns:\", list(df_raw.columns))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"userId\":    df_raw[\"User_id\"],\n",
    "    \"itemId\":    df_raw[\"Id\"],\n",
    "    \"rating\":    pd.to_numeric(df_raw[\"review/score\"], errors=\"coerce\"),\n",
    "    \"timestamp\": pd.to_datetime(df_raw[\"review/time\"], errors=\"coerce\"),\n",
    "})\n",
    "\n",
    "# Fill missing timestamps with a fixed dummy time and convert to Unix seconds\n",
    "df[\"timestamp\"] = df[\"timestamp\"].fillna(pd.Timestamp(2000, 1, 1))\n",
    "df[\"timestamp\"] = df[\"timestamp\"].astype(\"int64\") // 10**9\n",
    "\n",
    "# Keep only ratings in [1, 5]\n",
    "df = df[(df[\"rating\"] >= 1) & (df[\"rating\"] <= 5)]\n",
    "\n",
    "print(df.head(), df.dtypes)\n",
    "\n",
    "ratings_csv = BASE / \"ratings.csv\"\n",
    "df.to_csv(ratings_csv, index=False)\n",
    "print(f\"[saved] {ratings_csv} ~ {len(df):,} rows\")"
   ],
   "id": "a1f8094772a6aec0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw columns: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
      "           userId      itemId  rating  timestamp\n",
      "0   AVCGYZL8FQQTD  1882931173     4.0          0\n",
      "1  A30TK6U7DNS82R  0826414346     5.0          1\n",
      "2  A3UH4UZ4RSVO82  0826414346     5.0          1\n",
      "3  A2MVUWT453QH61  0826414346     4.0          1\n",
      "4  A22X4XUPKF66MR  0826414346     4.0          1 userId        object\n",
      "itemId        object\n",
      "rating       float64\n",
      "timestamp      int64\n",
      "dtype: object\n",
      "[saved] C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\ratings.csv ~ 3,000,000 rows\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Apply user–item k-core filter\n",
    "\n",
    "Next, I enforce a user–item k-core on `ratings.csv` so that every remaining user and item appears with at least 5 positive interactions.\n",
    "The k-core procedure iteratively prunes users and items with fewer than the required interactions until all surviving entries satisfy the minimum-degree constraint.\n",
    "This step removes extremely sparse users and rare items, stabilizing later splitting and similarity computations."
   ],
   "id": "8e467e597cd9fa2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:59:15.083833Z",
     "start_time": "2025-12-06T17:59:00.818131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def kcore_filter(df, u_col=\"userId\", i_col=\"itemId\", k_user=5, k_item=5, max_iters=20, verbose=True):\n",
    "    for it in range(max_iters):\n",
    "        n0, u0, i0 = len(df), df[u_col].nunique(), df[i_col].nunique()\n",
    "        uf = df[u_col].value_counts()\n",
    "        vf = df[i_col].value_counts()\n",
    "\n",
    "        df = df[df[u_col].isin(uf[uf >= k_user].index)]\n",
    "        df = df[df[i_col].isin(vf[vf >= k_item].index)]\n",
    "\n",
    "        n1, u1, i1 = len(df), df[u_col].nunique(), df[i_col].nunique()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[k-core {it+1}] rows {n0:,}->{n1:,}, users {u0:,}->{u1:,}, items {i0:,}->{i1:,}\")\n",
    "\n",
    "        if n1 == n0:\n",
    "            break\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the normalized ratings and apply k-core\n",
    "ratings_csv = BASE / \"ratings.csv\"\n",
    "df = pd.read_csv(ratings_csv)\n",
    "\n",
    "df = kcore_filter(df, k_user=5, k_item=5)\n",
    "print(f\"[after k-core] users={df['userId'].nunique():,}, items={df['itemId'].nunique():,}, rows={len(df):,}\")\n",
    "\n",
    "# Overwrite ratings.csv with the k-core filtered version\n",
    "df.to_csv(ratings_csv, index=False)\n",
    "print(f\"[saved] {ratings_csv} ~ {len(df):,} rows\")"
   ],
   "id": "f5bf3e0d8ca57d47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k-core 1] rows 3,000,000->1,077,091, users 1,008,972->82,519, items 221,998->69,986\n",
      "[k-core 2] rows 1,077,091->977,301, users 82,519->77,225, items 69,986->29,668\n",
      "[k-core 3] rows 977,301->951,522, users 77,225->70,381, items 29,668->28,742\n",
      "[k-core 4] rows 951,522->944,212, users 70,381->70,121, items 28,742->27,027\n",
      "[k-core 5] rows 944,212->941,724, users 70,121->69,534, items 27,027->26,952\n",
      "[k-core 6] rows 941,724->940,863, users 69,534->69,496, items 26,952->26,770\n",
      "[k-core 7] rows 940,863->940,525, users 69,496->69,417, items 26,770->26,757\n",
      "[k-core 8] rows 940,525->940,434, users 69,417->69,416, items 26,757->26,731\n",
      "[k-core 9] rows 940,434->940,416, users 69,416->69,411, items 26,731->26,731\n",
      "[k-core 10] rows 940,416->940,416, users 69,411->69,411, items 26,731->26,731\n",
      "[after k-core] users=69,411, items=26,731, rows=940,416\n",
      "[saved] C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\ratings.csv ~ 940,416 rows\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Enforce one canonical `itemId` per title\n",
    "\n",
    "To ensure that each indexed item corresponds to a single logical book, I collapse multiple raw Amazon IDs that share the same (fuzzily normalized) title into one canonical `itemId`.\n",
    "\n",
    "The steps are:\n",
    "1. Normalize titles using lowercasing and simple text cleaning rules to group visually similar variants.\n",
    "2. Build a mapping from normalized title to one canonical `itemId` (the smallest ID per title).\n",
    "3. Remap all interactions to the canonical IDs and drop any rows that cannot be matched.\n",
    "4. Remove duplicate `(userId, itemId, timestamp)` rows introduced by merging multiple raw IDs into one.\n",
    "\n",
    "The resulting `ratings.csv` has a one-to-one mapping between canonical `itemId` and normalized book titles."
   ],
   "id": "604407d36a6d715c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:43:53.493965Z",
     "start_time": "2025-12-08T19:43:19.102952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "BASE = Path(\".\")\n",
    "RAW_CSV_PATH = BASE / \"Books_rating.csv\"\n",
    "RATINGS_PATH = BASE / \"ratings.csv\"\n",
    "\n",
    "def normalize_title_for_dedup(title: str) -> str:\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "    t = title.lower()\n",
    "\n",
    "    t = re.sub(r\"\\([^)]*\\)\", \" \", t)\n",
    "\n",
    "    t = re.sub(r\"[-–—/:;]\", \" \", t)\n",
    "\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)\n",
    "\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    STOPWORDS = {\n",
    "        \"the\", \"a\", \"an\", \"or\", \"and\",\n",
    "        \"box\", \"boxed\", \"set\", \"edition\", \"ed\",\n",
    "        \"volume\", \"vol\", \"volumes\",\n",
    "        \"collection\", \"classic\", \"classics\", \"trilogy\",\n",
    "        \"audio\", \"audiobook\", \"audiobooks\", \"recordings\",\n",
    "        \"penguin\", \"bantam\", \"pocket\", \"enriched\", \"brilliance\",\n",
    "        \"one\", \"voice\"\n",
    "    }\n",
    "    tokens = [w for w in t.split() if w not in STOPWORDS]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "assert RAW_CSV_PATH.exists(), f\"Missing raw ratings file: {RAW_CSV_PATH}\"\n",
    "assert RATINGS_PATH.exists(), f\"Missing k-core ratings file: {RATINGS_PATH}\"\n",
    "\n",
    "ratings = pd.read_csv(RATINGS_PATH)\n",
    "print(\"Before title & interaction dedup:\")\n",
    "print(\" users :\", ratings[\"userId\"].nunique())\n",
    "print(\" items :\", ratings[\"itemId\"].nunique())\n",
    "print(\" rows  :\", len(ratings))\n",
    "\n",
    "raw = pd.read_csv(RAW_CSV_PATH, usecols=[\"Id\", \"Title\"])\n",
    "raw = raw.rename(columns={\"Id\": \"itemId\", \"Title\": \"title\"})\n",
    "raw = raw.dropna(subset=[\"title\"])\n",
    "\n",
    "raw[\"title_norm\"] = raw[\"title\"].map(normalize_title_for_dedup)\n",
    "\n",
    "print(\"\\nTitle stats in raw metadata:\")\n",
    "print(\" Unique raw titles        :\", raw[\"title\"].nunique())\n",
    "print(\" Unique normalized (fuzzy):\", raw[\"title_norm\"].nunique())\n",
    "\n",
    "raw_item = (\n",
    "    raw\n",
    "    .sort_values([\"itemId\", \"title_norm\"])\n",
    "    .drop_duplicates(subset=[\"itemId\"], keep=\"first\")\n",
    ")\n",
    "\n",
    "title_to_canon = (\n",
    "    raw_item\n",
    "    .sort_values([\"title_norm\", \"itemId\"])\n",
    "    .drop_duplicates(subset=[\"title_norm\"], keep=\"first\")\n",
    "    .set_index(\"title_norm\")[\"itemId\"]\n",
    ")\n",
    "\n",
    "print(\" Unique normalized titles in mapping:\", title_to_canon.shape[0])\n",
    "\n",
    "id_to_title_norm = raw_item.set_index(\"itemId\")[\"title_norm\"]\n",
    "\n",
    "id_to_canon = id_to_title_norm.map(title_to_canon)\n",
    "\n",
    "ratings[\"title_norm\"] = ratings[\"itemId\"].map(id_to_title_norm)\n",
    "missing_title_norm = ratings[\"title_norm\"].isna().sum()\n",
    "print(\"\\nRows in ratings missing title_norm from metadata:\", missing_title_norm)\n",
    "ratings = ratings.dropna(subset=[\"title_norm\"])\n",
    "\n",
    "ratings[\"canonical_itemId\"] = ratings[\"itemId\"].map(id_to_canon)\n",
    "missing_canon = ratings[\"canonical_itemId\"].isna().sum()\n",
    "print(\"Rows in ratings missing canonical_itemId (should be 0 or very small):\", missing_canon)\n",
    "ratings = ratings.dropna(subset=[\"canonical_itemId\"])\n",
    "\n",
    "ratings[\"itemId\"] = ratings[\"canonical_itemId\"]\n",
    "\n",
    "ratings = ratings.drop(columns=[\"canonical_itemId\", \"title_norm\"])\n",
    "\n",
    "ratings = (\n",
    "    ratings\n",
    "    .sort_values([\"userId\", \"itemId\", \"timestamp\"])\n",
    "    .drop_duplicates(subset=[\"userId\", \"itemId\"], keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nAfter FUZZY title + interaction dedup:\")\n",
    "print(\" users :\", ratings[\"userId\"].nunique())\n",
    "print(\" items :\", ratings[\"itemId\"].nunique())\n",
    "print(\" rows  :\", len(ratings))\n",
    "\n",
    "dup_interactions = ratings.duplicated(subset=[\"userId\", \"itemId\", \"timestamp\"]).sum()\n",
    "print(\"Duplicate interactions remaining (should be 0):\", dup_interactions)\n",
    "\n",
    "ratings.to_csv(RATINGS_PATH, index=False)\n",
    "print(f\"\\nSaved deduplicated ratings with fuzzy titles to {RATINGS_PATH}\")"
   ],
   "id": "f408367d1a43bb50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before title & interaction dedup:\n",
      " users : 69411\n",
      " items : 20500\n",
      " rows  : 495535\n",
      "\n",
      "Title stats in raw metadata:\n",
      " Unique raw titles        : 212403\n",
      " Unique normalized (fuzzy): 198553\n",
      " Unique normalized titles in mapping: 198553\n",
      "\n",
      "Rows in ratings missing title_norm from metadata: 0\n",
      "Rows in ratings missing canonical_itemId (should be 0 or very small): 0\n",
      "\n",
      "After FUZZY title + interaction dedup:\n",
      " users : 69411\n",
      " items : 20500\n",
      " rows  : 495535\n",
      "Duplicate interactions remaining (should be 0): 0\n",
      "\n",
      "Saved deduplicated ratings with fuzzy titles to ratings.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Deduplication validation and metadata coverage\n",
    "\n",
    "After fuzzy title deduplication, I run several sanity checks to confirm that the interaction table and the raw metadata stay consistent:\n",
    "\n",
    "1. Count how many normalized titles still map to multiple raw `itemId` values, and verify that each normalized title corresponds to exactly one canonical ID.\n",
    "2. Check that there are no remaining duplicate `(userId, itemId, timestamp)` rows after collapsing IDs.\n",
    "3. Confirm that every `itemId` appearing in `ratings.csv` has a corresponding entry in the raw metadata file.\n",
    "\n",
    "These checks provide evidence that the fuzzy title merge did not introduce conflicts or orphan items."
   ],
   "id": "d6ac522058705ba8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:43:59.493731Z",
     "start_time": "2025-12-08T19:43:57.882036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nDEDUP VALIDATION\\n\")\n",
    "\n",
    "tn_to_n_ids = (\n",
    "    raw_item\n",
    "    .groupby(\"title_norm\")[\"itemId\"]\n",
    "    .nunique()\n",
    ")\n",
    "multi_id_norms = (tn_to_n_ids > 1).sum()\n",
    "print(\"title_norm groups with MULTIPLE raw itemIds in raw_item (EXPECTED, shows fuzzy-merged groups):\", int(multi_id_norms))\n",
    "\n",
    "df_check = raw_item.copy()\n",
    "df_check[\"canonical_itemId\"] = df_check[\"title_norm\"].map(title_to_canon)\n",
    "n_canon_per_norm = (\n",
    "    df_check.groupby(\"title_norm\")[\"canonical_itemId\"]\n",
    "    .nunique()\n",
    ")\n",
    "bad_norms = (n_canon_per_norm > 1).sum()\n",
    "print(\"title_norm groups mapping to >1 canonical_itemId (should be 0):\",\n",
    "      int(bad_norms))\n",
    "\n",
    "dup_ui = ratings.duplicated(subset=[\"userId\", \"itemId\"]).sum()\n",
    "dup_uit = ratings.duplicated(subset=[\"userId\", \"itemId\", \"timestamp\"]).sum()\n",
    "print(\"Duplicate (userId, itemId) rows in ratings (should be 0):\",\n",
    "      int(dup_ui))\n",
    "print(\"Duplicate (userId, itemId, timestamp) rows in ratings (should be 0):\",\n",
    "      int(dup_uit))\n",
    "\n",
    "missing_meta = (~ratings[\"itemId\"].isin(raw_item[\"itemId\"])).sum()\n",
    "print(\"ItemIds in ratings WITHOUT metadata (should be 0):\",\n",
    "      int(missing_meta))\n",
    "\n",
    "print(\"\\nFinal stats after all dedup:\")\n",
    "print(\" users :\", ratings[\"userId\"].nunique())\n",
    "print(\" items :\", ratings[\"itemId\"].nunique())\n",
    "print(\" rows  :\", len(ratings))"
   ],
   "id": "6e5d574c1166e188",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEDUP VALIDATION\n",
      "\n",
      "title_norm groups with MULTIPLE raw itemIds in raw_item (EXPECTED, shows fuzzy-merged groups): 15588\n",
      "title_norm groups mapping to >1 canonical_itemId (should be 0): 0\n",
      "Duplicate (userId, itemId) rows in ratings (should be 0): 0\n",
      "Duplicate (userId, itemId, timestamp) rows in ratings (should be 0): 0\n",
      "ItemIds in ratings WITHOUT metadata (should be 0): 0\n",
      "\n",
      "Final stats after all dedup:\n",
      " users : 69411\n",
      " items : 20500\n",
      " rows  : 495535\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:00:40.548960Z",
     "start_time": "2025-12-06T18:00:40.242698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_norm = raw[\"title_norm\"].sample(1).iloc[0]\n",
    "print(\"title_norm =\", sample_norm)\n",
    "\n",
    "raw[raw[\"title_norm\"] == sample_norm][[\"itemId\", \"title\"]].head(50)"
   ],
   "id": "764cb0362651a4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_norm = 125 best gluten free recipes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             itemId                         title\n",
       "1027645  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027646  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027647  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027648  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027649  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027650  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027651  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027652  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027653  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027654  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027655  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027656  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027657  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027658  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027659  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027660  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027661  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027662  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027663  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027664  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027665  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027666  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027667  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027668  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027669  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027670  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027671  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027672  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES\n",
       "1027673  B000N6KP5Q  125 BEST GLUTEN FREE RECIPES"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemId</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1027645</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027646</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027647</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027648</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027649</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027650</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027651</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027652</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027653</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027654</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027655</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027656</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027657</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027658</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027659</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027660</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027661</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027662</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027663</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027664</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027665</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027666</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027667</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027668</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027669</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027670</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027671</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027672</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027673</th>\n",
       "      <td>B000N6KP5Q</td>\n",
       "      <td>125 BEST GLUTEN FREE RECIPES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:05:57.728591Z",
     "start_time": "2025-12-06T18:05:56.684583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "item_meta = raw[[\"itemId\", \"title\"]].drop_duplicates()\n",
    "\n",
    "ratings_with_title = ratings.merge(item_meta, on=\"itemId\", how=\"left\")\n",
    "\n",
    "dup_titles = (\n",
    "    ratings_with_title\n",
    "    .groupby(\"itemId\")[\"title\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"n_titles\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"How many canonical_itemId still have >1 distinct raw titles:\",\n",
    "    (dup_titles[\"n_titles\"] > 1).sum()\n",
    ")\n",
    "\n",
    "dup_titles[dup_titles[\"n_titles\"] > 1].head(20)"
   ],
   "id": "f630561fbc266292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many canonical_itemId still have >1 distinct raw titles: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [itemId, n_titles]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemId</th>\n",
       "      <th>n_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:00:59.134346Z",
     "start_time": "2025-12-06T18:00:58.829025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dup_full = ratings.groupby([\"userId\", \"itemId\", \"timestamp\"]).size()\n",
    "dup_full[dup_full > 1].head()"
   ],
   "id": "5ee03b53c85c216",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:01:01.361765Z",
     "start_time": "2025-12-06T18:01:01.070833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "missing_meta = set(ratings[\"itemId\"]) - set(raw[\"itemId\"])\n",
    "print(\"ItemIds missing metadata:\", len(missing_meta))"
   ],
   "id": "f66d27b1aad167d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemIds missing metadata: 0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4 Manual inspection of merged title groups\n",
    "\n",
    "To visually confirm the quality of the fuzzy title merge, I randomly sample a few normalized titles and print:\n",
    "\n",
    "1. All raw titles that map into each sampled normalized title.\n",
    "2. The chosen canonical `itemId` for that group.\n",
    "3. The number of distinct users and interactions that now refer to the canonical item.\n",
    "\n",
    "These spot-checks help verify that obviously unrelated books are not being merged, and that the canonical ID indeed aggregates closely related variants of the same title."
   ],
   "id": "7ef1a462f1ecc5f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T19:48:27.566464Z",
     "start_time": "2025-12-08T19:47:55.218954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw = pd.read_csv(RAW_CSV_PATH, usecols=[\"Id\", \"Title\"])\n",
    "raw = raw.rename(columns={\"Id\": \"itemId\", \"Title\": \"title\"})\n",
    "raw = raw.dropna(subset=[\"title\"])\n",
    "\n",
    "raw[\"title_norm\"] = raw[\"title\"].map(normalize_title_for_dedup)\n",
    "\n",
    "print(\"Unique raw titles        :\", raw[\"title\"].nunique())\n",
    "print(\"Unique normalized titles :\", raw[\"title_norm\"].nunique())\n",
    "print()\n",
    "\n",
    "canon_for_norm = (\n",
    "    raw.sort_values([\"title_norm\", \"itemId\"])\n",
    "       .drop_duplicates(subset=\"title_norm\", keep=\"first\")[[\"title_norm\", \"itemId\"]]\n",
    "       .rename(columns={\"itemId\": \"canonical_itemId\"})\n",
    ")\n",
    "\n",
    "norm_to_canon = dict(\n",
    "    zip(canon_for_norm[\"title_norm\"], canon_for_norm[\"canonical_itemId\"])\n",
    ")\n",
    "\n",
    "N_SAMPLES = 5\n",
    "\n",
    "sample_norms = (\n",
    "    canon_for_norm[\"title_norm\"]\n",
    "    .dropna()\n",
    "    .sample(N_SAMPLES, random_state=42)\n",
    "    .tolist()\n",
    ")\n",
    "print(\"MANUAL DEDUP INSPECTION\\n\")\n",
    "print(f\"Sampled {N_SAMPLES} title_norm values:\\n\", sample_norms, \"\\n\")\n",
    "\n",
    "for tnorm in sample_norms:\n",
    "    print(f\"title_norm = {tnorm!r}\")\n",
    "\n",
    "    block_raw = raw[raw[\"title_norm\"] == tnorm].sort_values(\"title\")\n",
    "    print(\"\\nRaw titles for this title_norm (before dedup):\")\n",
    "    print(\n",
    "        block_raw[[\"itemId\", \"title\"]]\n",
    "        .drop_duplicates()\n",
    "        .head(20)\n",
    "        .to_string(index=False)\n",
    "    )\n",
    "\n",
    "    canon_id = norm_to_canon[tnorm]\n",
    "    print(f\"\\nChosen canonical_itemId : {canon_id}\")\n",
    "\n",
    "    block_ratings = ratings[ratings[\"itemId\"] == canon_id]\n",
    "\n",
    "    n_users = block_ratings[\"userId\"].nunique()\n",
    "    n_rows  = len(block_ratings)\n",
    "\n",
    "    print(\n",
    "        f\"In cleaned ratings: {n_rows} interactions, \"\n",
    "        f\"{n_users} unique users, itemId = {canon_id}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "print(\"\\nDone. For each sampled title_norm, should see：\")\n",
    "print(\"There are pbbly a lot of similar titles merged together in raw\")\n",
    "print(\"There is only one itemId（canonical_itemId）kept in cleaned ratings\")"
   ],
   "id": "e74b071c764b9b54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique raw titles        : 212403\n",
      "Unique normalized titles : 198553\n",
      "\n",
      "MANUAL DEDUP INSPECTION\n",
      "\n",
      "Sampled 5 title_norm values:\n",
      " ['shoot everything you ever wanted to know about 35mm photography', 'edmund s used cars trucks prices ratings 1999 fall u3303', 'epic role playing rules manual', 'pictorial tribute to crewe works in age of steam', 'mexican cooking at academy at academy'] \n",
      "\n",
      "title_norm = 'shoot everything you ever wanted to know about 35mm photography'\n",
      "\n",
      "Raw titles for this title_norm (before dedup):\n",
      "    itemId                                                             title\n",
      "0817458697 Shoot!: Everything you Ever Wanted to Know About 35Mm Photography\n",
      "\n",
      "Chosen canonical_itemId : 0817458697\n",
      "In cleaned ratings: 0 interactions, 0 unique users, itemId = 0817458697\n",
      "\n",
      "title_norm = 'edmund s used cars trucks prices ratings 1999 fall u3303'\n",
      "\n",
      "Raw titles for this title_norm (before dedup):\n",
      "    itemId                                                                            title\n",
      "087759645X Edmund's Used Cars & Trucks: Prices & Ratings 1999 : Fall Vol U3303 (4 Per Year)\n",
      "\n",
      "Chosen canonical_itemId : 087759645X\n",
      "In cleaned ratings: 0 interactions, 0 unique users, itemId = 087759645X\n",
      "\n",
      "title_norm = 'epic role playing rules manual'\n",
      "\n",
      "Raw titles for this title_norm (before dedup):\n",
      "    itemId                           title\n",
      "0976094606 Epic Role Playing Rules Manual:\n",
      "\n",
      "Chosen canonical_itemId : 0976094606\n",
      "In cleaned ratings: 0 interactions, 0 unique users, itemId = 0976094606\n",
      "\n",
      "title_norm = 'pictorial tribute to crewe works in age of steam'\n",
      "\n",
      "Raw titles for this title_norm (before dedup):\n",
      "    itemId                                                  title\n",
      "0860933954 A Pictorial Tribute to Crewe Works in the Age of Steam\n",
      "\n",
      "Chosen canonical_itemId : 0860933954\n",
      "In cleaned ratings: 0 interactions, 0 unique users, itemId = 0860933954\n",
      "\n",
      "title_norm = 'mexican cooking at academy at academy'\n",
      "\n",
      "Raw titles for this title_norm (before dedup):\n",
      "    itemId                                                                               title\n",
      "1564260402 Mexican Cooking at the Academy: At the Academy (California Culinary Academy Series)\n",
      "\n",
      "Chosen canonical_itemId : 1564260402\n",
      "In cleaned ratings: 0 interactions, 0 unique users, itemId = 1564260402\n",
      "\n",
      "\n",
      "Done. For each sampled title_norm, should see：\n",
      "There are pbbly a lot of similar titles merged together in raw\n",
      "There is only one itemId（canonical_itemId）kept in cleaned ratings\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.5 Interaction statistics and positive/negative thresholds\n",
    "\n",
    "Before splitting the data, I analyze how many interactions each user has and how different rating thresholds affect the number of “positive” events.\n",
    "\n",
    "For several candidate thresholds (2.0, 3.0, 4.0, 5.0), I compute:\n",
    "1. How many users would retain at least a given number of positives.\n",
    "2. The total counts of positive versus negative interactions.\n",
    "\n",
    "This exploration motivates the later choice of a high positive threshold (`rating ≥ 5`) and a minimum number of positives per user when constructing training and evaluation splits."
   ],
   "id": "5a602ea33f50cb5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:06.719306Z",
     "start_time": "2025-12-06T18:09:06.056920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load final deduplicated ratings BEFORE LOO split\n",
    "ratings = pd.read_csv(BASE / \"ratings.csv\")\n",
    "\n",
    "print(\"Total users in ratings.csv:\", ratings[\"userId\"].nunique())\n",
    "print(\"Total interactions:\", len(ratings))\n",
    "\n",
    "# Compute number of interactions per user once (for later use)\n",
    "user_inter_count = ratings.groupby(\"userId\").size()\n",
    "\n",
    "print(\"\\nUsers with >=4 total interactions:\", (user_inter_count >= 4).sum())\n",
    "print(\"Users with >=5 total interactions:\", (user_inter_count >= 5).sum())\n",
    "print(\"Users with >=10 total interactions:\", (user_inter_count >= 10).sum())\n",
    "\n",
    "# Count positive/negative interactions under different thresholds\n",
    "for th in [2.0, 3.0, 4.0,5.0]:\n",
    "    print(f\" Threshold = {th}\")\n",
    "\n",
    "    # Positive and negative definitions\n",
    "    pos = ratings[ratings[\"rating\"] >= th]\n",
    "    neg = ratings[ratings[\"rating\"] < th]\n",
    "\n",
    "    # Per-user positive counts\n",
    "    cnt_pos = pos.groupby(\"userId\").size()\n",
    "\n",
    "    print(\"Users with >=2 positives:\", (cnt_pos >= 2).sum())\n",
    "    print(\"Users with >=3 positives:\", (cnt_pos >= 3).sum())\n",
    "    print(\"Users with >=5 positives:\", (cnt_pos >= 5).sum())\n",
    "    print(\"Total users considered (have >=1 positive):\", cnt_pos.size)\n",
    "\n",
    "    print(\"Total positive interactions:\", len(pos))\n",
    "    print(\"Total negative interactions:\", len(neg))\n",
    "\n",
    "print(\"\\nDone.\")"
   ],
   "id": "b5e1e8e73214b133",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users in ratings.csv: 69411\n",
      "Total interactions: 495535\n",
      "\n",
      "Users with >=4 total interactions: 38336\n",
      "Users with >=5 total interactions: 28092\n",
      "Users with >=10 total interactions: 9824\n",
      " Threshold = 2.0\n",
      "Users with >=2 positives: 59887\n",
      "Users with >=3 positives: 49107\n",
      "Users with >=5 positives: 26232\n",
      "Total users considered (have >=1 positive): 67573\n",
      "Total positive interactions: 474616\n",
      "Total negative interactions: 20919\n",
      " Threshold = 3.0\n",
      "Users with >=2 positives: 58206\n",
      "Users with >=3 positives: 47197\n",
      "Users with >=5 positives: 24398\n",
      "Total users considered (have >=1 positive): 66302\n",
      "Total positive interactions: 449362\n",
      "Total negative interactions: 46173\n",
      " Threshold = 4.0\n",
      "Users with >=2 positives: 54883\n",
      "Users with >=3 positives: 43486\n",
      "Users with >=5 positives: 21016\n",
      "Total users considered (have >=1 positive): 63823\n",
      "Total positive interactions: 393215\n",
      "Total negative interactions: 102320\n",
      " Threshold = 5.0\n",
      "Users with >=2 positives: 44822\n",
      "Users with >=3 positives: 33538\n",
      "Users with >=5 positives: 14064\n",
      "Total users considered (have >=1 positive): 55847\n",
      "Total positive interactions: 269476\n",
      "Total negative interactions: 226059\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Create time-aware leave-one-out (LOO) split\n",
    "\n",
    "Using the cleaned `ratings.csv`, I build a time-aware leave-one-out split:\n",
    "\n",
    "1. Convert timestamps into a consistent datetime series, automatically detecting whether the input is in seconds or milliseconds.\n",
    "2. Keep only interactions with `rating ≥ 5.0` as positives.\n",
    "3. For each user with at least 5 positives, sort interactions by time and label:\n",
    "   1. the most recent item as the **test** target,\n",
    "   2. the second most recent as the **validation** target,\n",
    "   3. all earlier positives as **train** history.\n",
    "4. Save train histories and val/test targets as Parquet files under `splits/`, along with contiguous user and item index maps.\n",
    "\n",
    "The resulting splits preserve temporal order and avoid information leakage from future interactions."
   ],
   "id": "82c68c091e1012fc"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:21.279737Z",
     "start_time": "2025-12-06T18:09:21.264956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _detect_ts_unit(ts_series: pd.Series) -> str:\n",
    "    vmax = float(ts_series.max())\n",
    "    # Heuristic: if timestamps are very large, assume milliseconds\n",
    "    return \"ms\" if vmax > 1e12 else \"s\"\n",
    "\n",
    "def time_aware_loo_split(\n",
    "    ratings_csv: str,\n",
    "    out_dir: str,\n",
    "    rating_threshold: float = 5.0,\n",
    "    min_positives: int = 5,\n",
    "    also_csv: bool = False,\n",
    "):\n",
    "    out = Path(out_dir)\n",
    "    (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ratings = pd.read_csv(ratings_csv)\n",
    "\n",
    "    need = {\"userId\", \"itemId\", \"rating\", \"timestamp\"}\n",
    "    missing = need - set(ratings.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
    "\n",
    "    # Convert timestamp column to pandas datetime with auto unit detection\n",
    "    try:\n",
    "        unit = _detect_ts_unit(ratings[\"timestamp\"])\n",
    "        ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
    "    except Exception:\n",
    "        # Fallback: let pandas guess\n",
    "        ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=\"s\", origin=\"unix\", errors=\"ignore\")\n",
    "\n",
    "    # Keep only positive interactions\n",
    "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
    "\n",
    "    # Sort by (userId, timestamp)\n",
    "    pos = pos.sort_values([\"userId\", \"ts\"], kind=\"mergesort\").drop_duplicates(\n",
    "        [\"userId\", \"itemId\"], keep=\"first\"\n",
    "    )\n",
    "\n",
    "    # Filter users with at least `min_positives` positives\n",
    "    cnt = pos.groupby(\"userId\")[\"itemId\"].transform(\"size\")\n",
    "    pos = pos[cnt >= min_positives].copy()\n",
    "\n",
    "    # Sort again (just to be safe after filtering)\n",
    "    pos = pos.sort_values([\"userId\", \"ts\"], kind=\"mergesort\")\n",
    "\n",
    "    # Index within each user\n",
    "    pos[\"n\"] = pos.groupby(\"userId\")[\"userId\"].transform(\"size\")\n",
    "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
    "\n",
    "    pos[\"split\"] = \"train\"\n",
    "    pos.loc[pos[\"idx\"] == pos[\"n\"] - 1, \"split\"] = \"test\"\n",
    "    pos.loc[pos[\"idx\"] == pos[\"n\"] - 2, \"split\"] = \"val\"\n",
    "\n",
    "    train = pos[pos[\"split\"] == \"train\"][[\"userId\", \"itemId\", \"ts\"]].reset_index(drop=True)\n",
    "    val_targets = pos[pos[\"split\"] == \"val\"][[\"userId\", \"itemId\", \"ts\"]].reset_index(drop=True)\n",
    "    test_targets = pos[pos[\"split\"] == \"test\"][[\"userId\", \"itemId\", \"ts\"]].reset_index(drop=True)\n",
    "\n",
    "    # Build contiguous user/item ID maps\n",
    "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"])\n",
    "    uids[\"uid\"] = range(len(uids))\n",
    "\n",
    "    iids = pd.DataFrame(sorted(train[\"itemId\"].unique()), columns=[\"itemId\"])\n",
    "    iids[\"iid\"] = range(len(iids))\n",
    "\n",
    "    # Map val/test userId/itemId into the same index space\n",
    "    val_idx = (\n",
    "        val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "        .merge(iids, on=\"itemId\", how=\"left\")\n",
    "        .drop(columns=[\"itemId\"])\n",
    "    )\n",
    "\n",
    "    test_idx = (\n",
    "        test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "        .merge(iids, on=\"itemId\", how=\"left\")\n",
    "        .drop(columns=[\"itemId\"])\n",
    "    )\n",
    "\n",
    "    sp = out / \"splits\"\n",
    "    train.to_parquet(sp / \"train.parquet\", index=False)\n",
    "    if len(val_targets):\n",
    "        val_targets.to_parquet(sp / \"val_targets.parquet\", index=False)\n",
    "        val_idx.to_parquet(sp / \"val_targets_indexed.parquet\", index=False)\n",
    "    test_targets.to_parquet(sp / \"test_targets.parquet\", index=False)\n",
    "    test_idx.to_parquet(sp / \"test_targets_indexed.parquet\", index=False)\n",
    "\n",
    "    uids.to_parquet(sp / \"user_id_map.parquet\", index=False)\n",
    "    iids.to_parquet(sp / \"item_id_map.parquet\", index=False)\n",
    "\n",
    "    # Save train with indexed ids\n",
    "    train_idx = (\n",
    "        train.merge(uids, on=\"userId\", how=\"inner\")\n",
    "        .merge(iids, on=\"itemId\", how=\"inner\")\n",
    "        .drop(columns=[\"userId\", \"itemId\"])\n",
    "    )\n",
    "    train_idx.to_parquet(sp / \"train_indexed.parquet\", index=False)\n",
    "\n",
    "    if also_csv:\n",
    "        for p in [\n",
    "            \"train\",\n",
    "            \"val_targets\",\n",
    "            \"test_targets\",\n",
    "            \"user_id_map\",\n",
    "            \"item_id_map\",\n",
    "            \"train_indexed\",\n",
    "            \"val_targets_indexed\",\n",
    "            \"test_targets_indexed\",\n",
    "        ]:\n",
    "            df = pd.read_parquet(sp / f\"{p}.parquet\")\n",
    "            df.to_csv(sp / f\"{p}.csv\", index=False)\n",
    "\n",
    "    cold_val = int(val_idx[\"iid\"].isna().sum()) if len(val_idx) else 0\n",
    "    cold_test = int(test_idx[\"iid\"].isna().sum()) if len(test_idx) else 0\n",
    "\n",
    "    stats = f\"\"\"Time-aware LOO split summary\n",
    "Users (TRAIN map): {len(uids)}\n",
    "Items (TRAIN map): {len(iids)}\n",
    "TRAIN positives   : {len(train)}\n",
    "VAL users          : {val_idx[\"uid\"].nunique() if len(val_idx) else 0}\n",
    "TEST users         : {test_idx[\"uid\"].nunique() if len(test_idx) else 0}\n",
    "Cold-start VAL items : {cold_val}\n",
    "Cold-start TEST items: {cold_test}\n",
    "\"\"\"\n",
    "    (sp / \"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
    "    print(stats)"
   ],
   "id": "8319054b873fdc3f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:30.820285Z",
     "start_time": "2025-12-06T18:09:29.840681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "time_aware_loo_split(\n",
    "    ratings_csv=str(BASE / \"ratings.csv\"),\n",
    "    out_dir=str(BASE),\n",
    "    rating_threshold=5.0,\n",
    "    min_positives=5,\n",
    "    also_csv=False,\n",
    ")"
   ],
   "id": "bd0c01b46624dbf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-aware LOO split summary\n",
      "Users (TRAIN map): 14064\n",
      "Items (TRAIN map): 18783\n",
      "TRAIN positives   : 141530\n",
      "VAL users          : 14064\n",
      "TEST users         : 14064\n",
      "Cold-start VAL items : 477\n",
      "Cold-start TEST items: 1146\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Ensure validation and test items are seen in training\n",
    "\n",
    "After creating the time-aware splits, I filter out any validation or test targets whose items never appear in the training histories.\n",
    "Only users whose target items are included in the training item set are kept, and the filtered val/test target files overwrite the originals.\n",
    "\n",
    "This step removes cold-start items from evaluation and guarantees that every evaluated target item has been observed at least once in the training data."
   ],
   "id": "87f2b156ba5ab1e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:33.921259Z",
     "start_time": "2025-12-06T18:09:33.727142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "SPLITS = BASE / \"splits\"\n",
    "train_idx = pd.read_parquet(SPLITS/\"train_indexed.parquet\")\n",
    "val_idx   = pd.read_parquet(SPLITS/\"val_targets_indexed.parquet\")\n",
    "test_idx  = pd.read_parquet(SPLITS/\"test_targets_indexed.parquet\")\n",
    "\n",
    "train_items = set(train_idx[\"iid\"].unique())\n",
    "val_keep  = val_idx[val_idx[\"iid\"].isin(train_items)].copy()\n",
    "test_keep = test_idx[test_idx[\"iid\"].isin(train_items)].copy()\n",
    "\n",
    "val_keep.to_parquet(SPLITS/\"val_targets_indexed.parquet\", index=False)\n",
    "test_keep.to_parquet(SPLITS/\"test_targets_indexed.parquet\", index=False)\n",
    "print(\"[covered] kept val:\", len(val_keep), \" / test:\", len(test_keep))\n"
   ],
   "id": "5458cf22cb944ff6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[covered] kept val: 13587  / test: 12918\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Build labeled interactions and training-only history\n",
    "\n",
    "Using the full interaction table, I:\n",
    "\n",
    "1. Assign a binary label `pos_neg_label` based on `rating ≥ 5` (positive) versus `< 5` (negative).\n",
    "2. Tag all validation and test target `(userId, itemId)` pairs and mark them with an `is_target` flag.\n",
    "3. Save a global `interactions_lookup.csv` containing every labeled interaction plus the target flag.\n",
    "4. Remove val/test target rows to produce `interactions_for_training.csv`, which contains only non-target interactions.\n",
    "\n",
    "I also check per-user statistics to ensure most users have at least 5 interactions and at least one positive and one negative for training."
   ],
   "id": "680041ca2afe3ce4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:39.917376Z",
     "start_time": "2025-12-06T18:09:36.733372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(BASE)\n",
    "\n",
    "ratings_path = BASE / \"ratings.csv\"\n",
    "ratings = pd.read_csv(ratings_path)\n",
    "\n",
    "print(\"Loaded ratings.csv\")\n",
    "print(\"  users :\", ratings[\"userId\"].nunique())\n",
    "print(\"  items :\", ratings[\"itemId\"].nunique())\n",
    "print(\"  rows  :\", len(ratings))\n",
    "\n",
    "THRESHOLD = 5  # rating == 5 -> positive, <=4 -> negative\n",
    "ratings[\"pos_neg_label\"] = (ratings[\"rating\"] >= THRESHOLD).astype(int)\n",
    "\n",
    "print(\"\\nOverall label stats (before removing targets):\")\n",
    "print(\"  positives:\", (ratings[\"pos_neg_label\"] == 1).sum())\n",
    "print(\"  negatives:\", (ratings[\"pos_neg_label\"] == 0).sum())\n",
    "\n",
    "val_targets  = pd.read_parquet(BASE / \"splits/val_targets.parquet\")\n",
    "test_targets = pd.read_parquet(BASE / \"splits/test_targets.parquet\")\n",
    "\n",
    "val_pairs  = val_targets[[\"userId\", \"itemId\"]].drop_duplicates()\n",
    "test_pairs = test_targets[[\"userId\", \"itemId\"]].drop_duplicates()\n",
    "\n",
    "val_test_pairs = pd.concat([val_pairs, test_pairs], ignore_index=True).drop_duplicates()\n",
    "val_test_pairs[\"is_target\"] = 1\n",
    "\n",
    "print(\"\\nVal/test target pairs:\")\n",
    "print(\"  val targets :\", len(val_pairs))\n",
    "print(\"  test targets:\", len(test_pairs))\n",
    "print(\"  unique pairs:\", len(val_test_pairs))\n",
    "\n",
    "ratings = ratings.merge(val_test_pairs, on=[\"userId\", \"itemId\"], how=\"left\")\n",
    "ratings[\"is_target\"] = ratings[\"is_target\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Target flag in ratings:\")\n",
    "print(\"  rows with is_target=1:\", (ratings[\"is_target\"] == 1).sum())\n",
    "\n",
    "lookup_path = BASE / \"csv_export\" / \"interactions_lookup.csv\"\n",
    "lookup_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ratings.to_csv(lookup_path, index=False)\n",
    "print(\"\\nSaved interactions_lookup.csv to:\", lookup_path)\n",
    "\n",
    "train_interactions = ratings[ratings[\"is_target\"] == 0].copy()\n",
    "\n",
    "print(\"\\nAfter removing val/test targets:\")\n",
    "print(\"  users :\", train_interactions[\"userId\"].nunique())\n",
    "print(\"  items :\", train_interactions[\"itemId\"].nunique())\n",
    "print(\"  rows  :\", len(train_interactions))\n",
    "\n",
    "cnt_total = train_interactions.groupby(\"userId\").size()\n",
    "cnt_pos = train_interactions[train_interactions[\"pos_neg_label\"] == 1].groupby(\"userId\").size()\n",
    "cnt_neg = train_interactions[train_interactions[\"pos_neg_label\"] == 0].groupby(\"userId\").size()\n",
    "\n",
    "valid_users = [\n",
    "    u for u in cnt_total.index\n",
    "    if cnt_total.get(u, 0) >= 5\n",
    "       and cnt_pos.get(u, 0) >= 1\n",
    "       and cnt_neg.get(u, 0) >= 1\n",
    "]\n",
    "valid_users = set(valid_users)\n",
    "\n",
    "print(\"\\nUsers satisfying >=5 interactions and at least 1 pos & 1 neg:\")\n",
    "print(\"  count:\", len(valid_users))\n",
    "\n",
    "val_users = set(val_pairs[\"userId\"].unique())\n",
    "test_users = set(test_pairs[\"userId\"].unique())\n",
    "val_test_users = val_users | test_users\n",
    "\n",
    "missing_users = val_test_users - valid_users\n",
    "if missing_users:\n",
    "    print(\"\\nWARNING: some val/test users do not meet >=5 & pos/neg condition.\")\n",
    "    print(\"  number of such users:\", len(missing_users))\n",
    "    # Still include them so that LLM has history\n",
    "    valid_users |= missing_users\n",
    "else:\n",
    "    print(\"\\nAll val/test users already satisfy the condition.\")\n",
    "\n",
    "# Apply final user filter\n",
    "train_interactions = train_interactions[train_interactions[\"userId\"].isin(valid_users)].copy()\n",
    "\n",
    "print(\"\\nFinal training interactions stats:\")\n",
    "print(\"  users :\", train_interactions[\"userId\"].nunique())\n",
    "print(\"  items :\", train_interactions[\"itemId\"].nunique())\n",
    "print(\"  rows  :\", len(train_interactions))\n",
    "\n",
    "train_path = BASE / \"csv_export\" / \"interactions_for_training.csv\"\n",
    "train_interactions.to_csv(train_path, index=False)\n",
    "print(\"\\nSaved interactions_for_training.csv to:\", train_path)"
   ],
   "id": "275ee0f22537c1e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ratings.csv\n",
      "  users : 69411\n",
      "  items : 20500\n",
      "  rows  : 495535\n",
      "\n",
      "Overall label stats (before removing targets):\n",
      "  positives: 269476\n",
      "  negatives: 226059\n",
      "\n",
      "Val/test target pairs:\n",
      "  val targets : 14064\n",
      "  test targets: 14064\n",
      "  unique pairs: 28128\n",
      "Target flag in ratings:\n",
      "  rows with is_target=1: 28128\n",
      "\n",
      "Saved interactions_lookup.csv to: csv_export\\interactions_lookup.csv\n",
      "\n",
      "After removing val/test targets:\n",
      "  users : 69411\n",
      "  items : 20498\n",
      "  rows  : 467407\n",
      "\n",
      "Users satisfying >=5 interactions and at least 1 pos & 1 neg:\n",
      "  count: 21818\n",
      "\n",
      "WARNING: some val/test users do not meet >=5 & pos/neg condition.\n",
      "  number of such users: 4360\n",
      "\n",
      "Final training interactions stats:\n",
      "  users : 26178\n",
      "  items : 20435\n",
      "  rows  : 345571\n",
      "\n",
      "Saved interactions_for_training.csv to: csv_export\\interactions_for_training.csv\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Build implicit feedback matrix and item–item baseline\n",
    "\n",
    "From the indexed train split, I construct a sparse user–item implicit feedback matrix `R` based on the last 200 interactions per user.\n",
    "By computing `C = RᵀR`, I obtain an approximate item–item co-occurrence similarity structure and derive, for each item, a small list of its most similar neighbors.\n",
    "\n",
    "This `item_sim_map` can be used as a classical item–item recommendation baseline and later to build hybrid candidate pools that mix popularity and similarity-based candidates."
   ],
   "id": "2cbe91c78f7083c0"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:42.679023Z",
     "start_time": "2025-12-06T18:09:42.160286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "SPLITS = BASE / \"splits\"\n",
    "train_idx = pd.read_parquet(SPLITS/\"train_indexed.parquet\")   # [uid, iid, ts]\n",
    "val_idx   = pd.read_parquet(SPLITS/\"val_targets_indexed.parquet\")   # [uid, val_item(iid), ts_val]\n",
    "test_idx  = pd.read_parquet(SPLITS/\"test_targets_indexed.parquet\")  # [uid, test_item(iid), ts_test]\n",
    "\n",
    "U = int(train_idx[\"uid\"].max()) + 1\n",
    "I = int(train_idx[\"iid\"].max()) + 1\n",
    "\n",
    "user_seen = train_idx.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n",
    "\n",
    "def candidate_coverage(cand_df, targets_df, tgt_col=\"iid\"):\n",
    "    df = cand_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\", how=\"inner\")\n",
    "    df = df[df[tgt_col].notna()]\n",
    "    return np.mean([(int(t) in set(c)) for t, c in zip(df[tgt_col], df[\"candidates\"])])"
   ],
   "id": "ea1dd888e565b82f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:44.715300Z",
     "start_time": "2025-12-06T18:09:44.151522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_idx_sorted = train_idx.sort_values([\"uid\",\"ts\"]).groupby(\"uid\").tail(200)\n",
    "\n",
    "R = csr_matrix(\n",
    "    (np.ones(len(train_idx_sorted), dtype=np.float32),\n",
    "     (train_idx_sorted[\"uid\"].astype(int).values,\n",
    "      train_idx_sorted[\"iid\"].astype(int).values)),\n",
    "    shape=(U, I),\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "C = (R.T @ R).tocsr()\n",
    "C.setdiag(0); C.eliminate_zeros()\n",
    "\n",
    "M_SIM = 50\n",
    "item_sim_map = {}\n",
    "for iid in range(I):\n",
    "    a, b = C.indptr[iid], C.indptr[iid+1]\n",
    "    if a == b:\n",
    "        item_sim_map[iid] = []\n",
    "        continue\n",
    "    neigh = C.indices[a:b]\n",
    "    vals  = C.data[a:b]\n",
    "    if len(neigh) > M_SIM:\n",
    "        top = np.argpartition(-vals, M_SIM)[:M_SIM]\n",
    "        neigh, vals = neigh[top], vals[top]\n",
    "    order = np.argsort(-vals)\n",
    "    item_sim_map[iid] = neigh[order].tolist()\n",
    "\n",
    "print(f\"[item-sim] built for I={I:,}. Example of item 0:\", item_sim_map.get(0, [])[:10])"
   ],
   "id": "139b02f64124c0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[item-sim] built for I=18,783. Example of item 0: [7, 22, 26, 28, 77, 84, 202, 295, 477, 904]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Create 100-user candidate subset\n",
    "\n",
    "To keep LLM-based experiments computationally manageable, I select 100 users who have both validation and test targets.\n",
    "For these users, I build candidate lists by ranking items by global popularity and excluding items already seen in each user’s train history.\n",
    "The resulting candidate pools, along with their val/test targets, are saved under `candidates_subset100/` for later use."
   ],
   "id": "f2ceabb15c9b1202"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:47.671992Z",
     "start_time": "2025-12-06T18:09:47.586499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SPLITS = BASE / \"splits\"\n",
    "\n",
    "train_idx = pd.read_parquet(SPLITS / \"train_indexed.parquet\")\n",
    "val_idx   = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\")\n",
    "test_idx  = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")\n",
    "\n",
    "users_with_val  = set(val_idx['uid'].unique())\n",
    "users_with_test = set(test_idx['uid'].unique())\n",
    "candidate_users = sorted(users_with_val & users_with_test)\n",
    "\n",
    "subset_users = candidate_users[:100]\n",
    "\n",
    "print(\"Users with val+test:\", len(candidate_users))\n",
    "print(\"Selected 100 users:\", len(subset_users))\n",
    "\n",
    "train_sub = train_idx[train_idx['uid'].isin(subset_users)].copy()\n",
    "val_tgt   = val_idx[val_idx['uid'].isin(subset_users)].copy()\n",
    "test_tgt  = test_idx[test_idx['uid'].isin(subset_users)].copy()\n",
    "\n",
    "item_pop_series = train_sub['iid'].value_counts().astype(float)\n",
    "item_pop_series /= item_pop_series.max()\n",
    "item_pop = item_pop_series.to_dict()\n",
    "item_popular = sorted(item_pop.items(), key=lambda x: -x[1])\n",
    "\n",
    "def build_pool_for_user(uid, k=50):\n",
    "    seen = set(train_sub.loc[train_sub['uid'] == uid, 'iid'].astype(int))\n",
    "    pool = [int(i) for i, _ in item_popular if i not in seen][:k]\n",
    "    return pool\n",
    "\n",
    "rows = []\n",
    "for u in subset_users:\n",
    "    rows.append({\n",
    "        \"uid\": int(u),\n",
    "        \"candidates\": build_pool_for_user(int(u), k=50)\n",
    "    })\n",
    "\n",
    "cand = pd.DataFrame(rows)\n",
    "\n",
    "OUT = BASE / \"candidates_subset100\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Saving 100-user candidates to:\", OUT)\n",
    "\n",
    "cand.to_parquet(OUT / \"val.parquet\",  index=False)\n",
    "cand.to_parquet(OUT / \"test.parquet\", index=False)\n",
    "\n",
    "val_tgt.to_parquet(OUT / \"val_targets_indexed.parquet\",  index=False)\n",
    "test_tgt.to_parquet(OUT / \"test_targets_indexed.parquet\", index=False)\n",
    "\n",
    "print(\"✓ Saved 100-user subset to\", OUT)"
   ],
   "id": "3c31f1680ec89641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users with val+test: 12542\n",
      "Selected 100 users: 100\n",
      "Saving 100-user candidates to: candidates_subset100\n",
      "✓ Saved 100-user subset to candidates_subset100\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Force-add ground-truth items into candidate pools\n",
    "\n",
    "For the 100-user subset, I guarantee that each user’s validation and test target items are always included in their candidate lists.\n",
    "\n",
    "For each user:\n",
    "1. Load the initial popularity-based candidate list.\n",
    "2. Compute the union of candidate items with that user’s val/test target items.\n",
    "3. Write back updated candidate lists so that val and test now share identical candidate pools.\n",
    "\n",
    "This ensures that evaluation metrics are well-defined for every user and that targets are never missing from the candidate set."
   ],
   "id": "2f19e2138ce620ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:50.766676Z",
     "start_time": "2025-12-06T18:09:50.678464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\")\n",
    "CANDS = BASE / \"candidates_subset100\"\n",
    "\n",
    "cand_val  = pd.read_parquet(CANDS / \"val.parquet\")\n",
    "cand_test = pd.read_parquet(CANDS / \"test.parquet\")\n",
    "\n",
    "val_tgt   = pd.read_parquet(CANDS / \"val_targets_indexed.parquet\")\n",
    "test_tgt  = pd.read_parquet(CANDS / \"test_targets_indexed.parquet\")\n",
    "\n",
    "print(\"Loaded cand_val :\", cand_val.shape)\n",
    "print(\"Loaded cand_test:\", cand_test.shape)\n",
    "print(\"Loaded val_tgt  :\", val_tgt.shape)\n",
    "print(\"Loaded test_tgt :\", test_tgt.shape)\n",
    "\n",
    "cand_map = {int(row.uid): list(row.candidates) for _, row in cand_val.iterrows()}\n",
    "\n",
    "for uid in sorted(cand_map.keys()):\n",
    "    uid = int(uid)\n",
    "\n",
    "    val_items  = set(val_tgt[val_tgt[\"uid\"] == uid][\"iid\"].astype(int).tolist())\n",
    "    test_items = set(test_tgt[test_tgt[\"uid\"] == uid][\"iid\"].astype(int).tolist())\n",
    "    gt_items   = val_items | test_items\n",
    "\n",
    "    cur_list = cand_map[uid]\n",
    "    cur_set  = set(int(i) for i in cur_list)\n",
    "\n",
    "    added = 0\n",
    "    for g in gt_items:\n",
    "        g = int(g)\n",
    "        if g not in cur_set:\n",
    "            cur_list.append(g)\n",
    "            cur_set.add(g)\n",
    "            added += 1\n",
    "\n",
    "    cand_map[uid] = cur_list\n",
    "\n",
    "cand_fixed = pd.DataFrame(\n",
    "    [{\"uid\": uid, \"candidates\": cand_map[uid]} for uid in sorted(cand_map.keys())]\n",
    ")\n",
    "\n",
    "cand_fixed.to_parquet(CANDS / \"val.parquet\", index=False)\n",
    "cand_fixed.to_parquet(CANDS / \"test.parquet\", index=False)\n",
    "\n",
    "print(\"✓ Ground-truth items have been forced into candidate pools.\")\n",
    "print(\"  New cand_val shape :\", cand_fixed.shape)\n",
    "print(\"  (val / test now share exactly the same candidate pools)\")"
   ],
   "id": "d683c454d4a3799f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cand_val : (100, 2)\n",
      "Loaded cand_test: (100, 2)\n",
      "Loaded val_tgt  : (100, 4)\n",
      "Loaded test_tgt : (100, 4)\n",
      "✓ Ground-truth items have been forced into candidate pools.\n",
      "  New cand_val shape : (100, 2)\n",
      "  (val / test now share exactly the same candidate pools)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Build labeled candidate pools\n",
    "\n",
    "Using the candidate lists, the user/item index maps, and the global interaction lookup table, I construct final labeled candidate pools:\n",
    "\n",
    "1. Flatten the per-user candidate lists into a long table of `(uid, iid)` pairs.\n",
    "2. Join with the interaction labels to assign `label` (positive/negative) for each candidate when available.\n",
    "3. Mark which rows correspond to ground-truth targets via an `is_target` flag.\n",
    "4. Export separate CSV files `candidate_pool_val.csv` and `candidate_pool_test.csv` under the project root.\n",
    "\n",
    "These files are the main inputs for both traditional recommenders and LLM-based rerankers."
   ],
   "id": "29258727a72ca3cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:09:57.239891Z",
     "start_time": "2025-12-06T18:09:54.620877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(BASE)\n",
    "SPLITS = BASE / \"splits\"\n",
    "CANDS = BASE / \"candidates_subset100\"\n",
    "LOOKUP = BASE / \"csv_export\" / \"interactions_lookup.csv\"\n",
    "\n",
    "user_map = pd.read_parquet(SPLITS / \"user_id_map.parquet\")\n",
    "item_map = pd.read_parquet(SPLITS / \"item_id_map.parquet\")\n",
    "\n",
    "print(\"User map:\", user_map.shape)\n",
    "print(\"Item map:\", item_map.shape)\n",
    "\n",
    "lookup = pd.read_csv(LOOKUP)\n",
    "print(\"Lookup table:\", lookup.shape)\n",
    "\n",
    "lookup_uid = lookup.merge(user_map, on=\"userId\", how=\"left\")\n",
    "lookup_uid_iid = lookup_uid.merge(item_map, on=\"itemId\", how=\"left\")\n",
    "\n",
    "lookup_uid_iid = lookup_uid_iid[[\"uid\", \"iid\", \"pos_neg_label\"]].dropna()\n",
    "lookup_uid_iid[\"uid\"] = lookup_uid_iid[\"uid\"].astype(int)\n",
    "lookup_uid_iid[\"iid\"] = lookup_uid_iid[\"iid\"].astype(int)\n",
    "\n",
    "lookup_uid_iid.set_index([\"uid\", \"iid\"], inplace=True)\n",
    "\n",
    "\n",
    "def build_candidate_pool(split: str):\n",
    "    print(f\"\\n--- Building candidate pool for {split} ---\")\n",
    "\n",
    "    cand_path = CANDS / f\"{split}.parquet\"\n",
    "    print(\"Loading candidates from:\", cand_path)\n",
    "    candidates = pd.read_parquet(cand_path)\n",
    "    print(\"Candidates shape (raw):\", candidates.shape)\n",
    "    print(\"Candidate columns:\", list(candidates.columns))\n",
    "\n",
    "    rows = []\n",
    "    for _, row in candidates.iterrows():\n",
    "        uid = int(row[\"uid\"])\n",
    "        for iid in row[\"candidates\"]:\n",
    "            rows.append((uid, int(iid)))\n",
    "\n",
    "    cand_flat = pd.DataFrame(rows, columns=[\"uid\", \"iid\"])\n",
    "    print(\"Flattened candidates shape:\", cand_flat.shape)\n",
    "\n",
    "    tgt_path = SPLITS / f\"{split}_targets_indexed.parquet\"\n",
    "    tgt = pd.read_parquet(tgt_path)[[\"userId\", \"iid\"]].copy()\n",
    "\n",
    "    tgt = tgt.merge(user_map, on=\"userId\", how=\"left\")[[\"uid\", \"iid\"]]\n",
    "    tgt[\"uid\"] = tgt[\"uid\"].astype(int)\n",
    "    tgt[\"iid\"] = tgt[\"iid\"].astype(int)\n",
    "\n",
    "    tgt[\"is_target\"] = 1\n",
    "\n",
    "    cand_with_tgt = pd.concat([cand_flat, tgt[[\"uid\", \"iid\"]]], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    def get_label(row):\n",
    "        key = (row[\"uid\"], row[\"iid\"])\n",
    "        if key in lookup_uid_iid.index:\n",
    "            val = lookup_uid_iid.loc[key, \"pos_neg_label\"]\n",
    "            if isinstance(val, pd.Series):\n",
    "                val = val.iloc[0]\n",
    "            return int(val)\n",
    "        return 0\n",
    "\n",
    "    cand_with_tgt[\"label\"] = cand_with_tgt.apply(get_label, axis=1)\n",
    "\n",
    "    cand_with_tgt = cand_with_tgt.merge(\n",
    "        tgt[[\"uid\", \"iid\", \"is_target\"]],\n",
    "        on=[\"uid\", \"iid\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    cand_with_tgt[\"is_target\"] = cand_with_tgt[\"is_target\"].fillna(0).astype(int)\n",
    "    out = cand_with_tgt.merge(user_map, on=\"uid\", how=\"left\").merge(item_map, on=\"iid\", how=\"left\")\n",
    "    out_csv = BASE / f\"candidate_pool_{split}.csv\"\n",
    "    out[[\"userId\", \"itemId\", \"label\", \"is_target\"]].to_csv(out_csv, index=False)\n",
    "    print(f\"Saved candidate_pool_{split}.csv with shape:\", out.shape)\n",
    "\n",
    "for split in [\"val\", \"test\"]:\n",
    "    build_candidate_pool(split)\n",
    "print(\"\\nDone building candidate_pool_val.csv and candidate_pool_test.csv.\")"
   ],
   "id": "ccbbedadc088c5fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User map: (14064, 2)\n",
      "Item map: (18783, 2)\n",
      "Lookup table: (495535, 6)\n",
      "\n",
      "--- Building candidate pool for val ---\n",
      "Loading candidates from: C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\candidates_subset100\\val.parquet\n",
      "Candidates shape (raw): (100, 2)\n",
      "Candidate columns: ['uid', 'candidates']\n",
      "Flattened candidates shape: (5199, 2)\n",
      "Saved candidate_pool_val.csv with shape: (18686, 6)\n",
      "\n",
      "--- Building candidate pool for test ---\n",
      "Loading candidates from: C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\candidates_subset100\\test.parquet\n",
      "Candidates shape (raw): (100, 2)\n",
      "Candidate columns: ['uid', 'candidates']\n",
      "Flattened candidates shape: (5199, 2)\n",
      "Saved candidate_pool_test.csv with shape: (18017, 6)\n",
      "\n",
      "Done building candidate_pool_val.csv and candidate_pool_test.csv.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Truncate training histories to the last 5 positives per user\n",
    "\n",
    "To make user histories more comparable to typical sequence lengths and to limit context size for LLM prompts, I keep at most the 5 most recent positive interactions per user in `train_indexed.parquet`.\n",
    "\n",
    "The original full train file is backed up, and the truncated version overwrites `train_indexed.parquet`.\n",
    "I also verify minimum and maximum history lengths after truncation to ensure that users still retain enough context for modeling."
   ],
   "id": "1e723c78834b0306"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:10:00.111611Z",
     "start_time": "2025-12-06T18:10:00.026115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\")\n",
    "SPLITS = BASE / \"splits\"\n",
    "\n",
    "train_path = SPLITS / \"train_indexed.parquet\"\n",
    "train_idx = pd.read_parquet(train_path)\n",
    "\n",
    "print(\"Original train_indexed shape:\", train_idx.shape)\n",
    "\n",
    "old_counts = train_idx.groupby(\"uid\")[\"iid\"].size()\n",
    "print(\"\\n[Old] train positives per user:\")\n",
    "print(old_counts.describe())\n",
    "\n",
    "def truncate_history(df: pd.DataFrame, k: int = 5) -> pd.DataFrame:\n",
    "    df_sorted = df.sort_values([\"uid\", \"ts\"])\n",
    "    out = (\n",
    "        df_sorted\n",
    "        .groupby(\"uid\", as_index=False)\n",
    "        .tail(k)\n",
    "        .sort_values([\"uid\", \"ts\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "train_trunc = truncate_history(train_idx, k=5)\n",
    "\n",
    "new_counts = train_trunc.groupby(\"uid\")[\"iid\"].size()\n",
    "print(\"\\n[New] train positives per user (after truncation to <=5):\")\n",
    "print(new_counts.describe())\n",
    "\n",
    "print(\"\\nCheck min/new max history length per user:\")\n",
    "print(\"  min =\", int(new_counts.min()), \"  max =\", int(new_counts.max()))\n",
    "\n",
    "backup_path = SPLITS / \"train_indexed_full_backup.parquet\"\n",
    "if not backup_path.exists():\n",
    "    train_idx.to_parquet(backup_path, index=False)\n",
    "    print(\"\\nBackup saved to:\", backup_path)\n",
    "else:\n",
    "    print(\"\\nBackup already exists at:\", backup_path)\n",
    "\n",
    "train_trunc.to_parquet(train_path, index=False)\n",
    "print(\"\\n✔ Overwritten train_indexed.parquet with truncated history.\")\n",
    "print(\"Final train_indexed shape:\", train_trunc.shape)"
   ],
   "id": "8f52008f2fc5265",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_indexed shape: (141530, 3)\n",
      "\n",
      "[Old] train positives per user:\n",
      "count    14064.000000\n",
      "mean        10.063282\n",
      "std         23.043830\n",
      "min          3.000000\n",
      "25%          3.000000\n",
      "50%          5.000000\n",
      "75%          9.000000\n",
      "max       1456.000000\n",
      "Name: iid, dtype: float64\n",
      "\n",
      "[New] train positives per user (after truncation to <=5):\n",
      "count    14064.000000\n",
      "mean         4.304323\n",
      "std          0.856873\n",
      "min          3.000000\n",
      "25%          3.000000\n",
      "50%          5.000000\n",
      "75%          5.000000\n",
      "max          5.000000\n",
      "Name: iid, dtype: float64\n",
      "\n",
      "Check min/new max history length per user:\n",
      "  min = 3   max = 5\n",
      "\n",
      "Backup saved to: C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\splits\\train_indexed_full_backup.parquet\n",
      "\n",
      "✔ Overwritten train_indexed.parquet with truncated history.\n",
      "Final train_indexed shape: (60536, 3)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Sanity checks on 100-user subset and candidate pools\n",
    "\n",
    "I run a final set of checks on the 100-user subset to ensure:\n",
    "\n",
    "1. The same 100 users appear consistently in train, validation, and test targets.\n",
    "2. Per-user interaction counts in the truncated train split look reasonable.\n",
    "3. Each user has exactly one candidate row in both `val` and `test` candidate files.\n",
    "4. Validation and test candidate pools are identical for each user (same `uid` order and same candidate lists).\n",
    "\n",
    "These checks confirm that the subset is internally consistent before exporting it for experiments."
   ],
   "id": "a901d268501181a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:10:03.560012Z",
     "start_time": "2025-12-06T18:10:03.532283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\")\n",
    "SPLITS = BASE / \"splits\"\n",
    "CANDS  = BASE / \"candidates_subset100\"\n",
    "\n",
    "train_idx = pd.read_parquet(SPLITS / \"train_indexed.parquet\")\n",
    "val_idx_sub  = pd.read_parquet(CANDS / \"val_targets_indexed.parquet\")\n",
    "test_idx_sub = pd.read_parquet(CANDS / \"test_targets_indexed.parquet\")\n",
    "\n",
    "cand_val  = pd.read_parquet(CANDS / \"val.parquet\")\n",
    "cand_test = pd.read_parquet(CANDS / \"test.parquet\")\n",
    "\n",
    "print(\"Loaded train_idx:\", train_idx.shape)\n",
    "print(\"Loaded val_idx_sub:\",  val_idx_sub.shape)\n",
    "print(\"Loaded test_idx_sub:\", test_idx_sub.shape)\n",
    "print(\"Loaded cand_val:\",  cand_val.shape)\n",
    "print(\"Loaded cand_test:\", cand_test.shape)\n",
    "\n",
    "users_train = set(train_idx['uid'].unique())\n",
    "users_val   = set(val_idx_sub['uid'].unique())\n",
    "users_test  = set(test_idx_sub['uid'].unique())\n",
    "\n",
    "subset_users = users_val\n",
    "print(\"\\n[1] User counts\")\n",
    "print(\"train users in subset:\", len(users_train & subset_users))\n",
    "print(\"val users:\",  len(users_val))\n",
    "print(\"test users:\", len(users_test))\n",
    "\n",
    "train_sub = train_idx[train_idx['uid'].isin(subset_users)]\n",
    "\n",
    "train_counts = train_sub.groupby(\"uid\")['iid'].nunique()\n",
    "val_counts   = val_idx_sub.groupby(\"uid\")['iid'].nunique()\n",
    "test_counts  = test_idx_sub.groupby(\"uid\")['iid'].nunique()\n",
    "\n",
    "print(\"\\n[2] Per-user counts (subset)\")\n",
    "print(\"Train counts stats:\\n\", train_counts.describe())\n",
    "print(\"Val counts unique values:\",  val_counts.unique())\n",
    "print(\"Test counts unique values:\", test_counts.unique())\n",
    "\n",
    "print(\"\\n[3] Candidate rows per user\")\n",
    "print(\"val candidate users:\",  cand_val['uid'].nunique())\n",
    "print(\"test candidate users:\", cand_test['uid'].nunique())\n",
    "\n",
    "cand_val_sorted  = cand_val.sort_values(\"uid\").reset_index(drop=True)\n",
    "cand_test_sorted = cand_test.sort_values(\"uid\").reset_index(drop=True)\n",
    "\n",
    "same_uid_order = (cand_val_sorted['uid'].tolist() ==\n",
    "                  cand_test_sorted['uid'].tolist())\n",
    "same_pools = all(\n",
    "    list(v) == list(t)\n",
    "    for v, t in zip(cand_val_sorted['candidates'],\n",
    "                    cand_test_sorted['candidates'])\n",
    ")\n",
    "\n",
    "print(\"\\n[4] Same candidate pool for val & test?\")\n",
    "print(\"same uid order :\", same_uid_order)\n",
    "print(\"same pools     :\", same_pools)"
   ],
   "id": "b8327673ca8ae29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_idx: (60536, 3)\n",
      "Loaded val_idx_sub: (100, 4)\n",
      "Loaded test_idx_sub: (100, 4)\n",
      "Loaded cand_val: (100, 2)\n",
      "Loaded cand_test: (100, 2)\n",
      "\n",
      "[1] User counts\n",
      "train users in subset: 100\n",
      "val users: 100\n",
      "test users: 100\n",
      "\n",
      "[2] Per-user counts (subset)\n",
      "Train counts stats:\n",
      " count    100.000000\n",
      "mean       4.090000\n",
      "std        0.911154\n",
      "min        3.000000\n",
      "25%        3.000000\n",
      "50%        4.000000\n",
      "75%        5.000000\n",
      "max        5.000000\n",
      "Name: iid, dtype: float64\n",
      "Val counts unique values: [1]\n",
      "Test counts unique values: [1]\n",
      "\n",
      "[3] Candidate rows per user\n",
      "val candidate users: 100\n",
      "test candidate users: 100\n",
      "\n",
      "[4] Same candidate pool for val & test?\n",
      "same uid order : True\n",
      "same pools     : True\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Export final splits and candidate pools to CSV\n",
    "\n",
    "Once all checks pass, I export the key Parquet artifacts to CSV under `csv_export/`:\n",
    "\n",
    "1. `train_indexed.csv`, `val_targets_indexed.csv`, `test_targets_indexed.csv`\n",
    "2. `user_id_map.csv`, `item_id_map.csv`\n",
    "3. `candidates_val.csv`, `candidates_test.csv`\n",
    "\n",
    "These CSV files are the main interface for downstream models, notebooks, and LLM prompting pipelines."
   ],
   "id": "6e84e6c24f8eb207"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:10:07.827894Z",
     "start_time": "2025-12-06T18:10:07.547808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"C:/Users/carlk/OneDrive/Documents/uoft/ECE1508H F/Project\")\n",
    "SPLITS = BASE / \"splits\"\n",
    "CAND = BASE / \"candidates_subset100\"\n",
    "\n",
    "OUT = BASE / \"csv_export\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Loading latest (TRUNCATED) splits...\")\n",
    "\n",
    "train_idx = pd.read_parquet(SPLITS / \"train_indexed.parquet\")\n",
    "val_tgt   = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\")\n",
    "test_tgt  = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")\n",
    "\n",
    "user_map  = pd.read_parquet(SPLITS / \"user_id_map.parquet\")\n",
    "item_map  = pd.read_parquet(SPLITS / \"item_id_map.parquet\")\n",
    "\n",
    "cand_val  = pd.read_parquet(CAND / \"val.parquet\")\n",
    "cand_test = pd.read_parquet(CAND / \"test.parquet\")\n",
    "\n",
    "train_idx.to_csv(OUT / \"train_indexed.csv\", index=False)\n",
    "val_tgt.to_csv(OUT / \"val_targets_indexed.csv\", index=False)\n",
    "test_tgt.to_csv(OUT / \"test_targets_indexed.csv\", index=False)\n",
    "\n",
    "user_map.to_csv(OUT / \"user_id_map.csv\", index=False)\n",
    "item_map.to_csv(OUT / \"item_id_map.csv\", index=False)\n",
    "\n",
    "cand_val.to_csv(OUT / \"candidates_val.csv\", index=False)\n",
    "cand_test.to_csv(OUT / \"candidates_test.csv\", index=False)\n",
    "\n",
    "print(\"\\n✓ All CSV exported to:\", OUT)"
   ],
   "id": "fc6003f880cb9e85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest (TRUNCATED) splits...\n",
      "\n",
      "✓ All CSV exported to: C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\csv_export\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Build simple item-level metadata (title + one review field)\n",
    "\n",
    "For a lightweight metadata table, I join the `item_id_map` with the original Amazon CSV to obtain, for each indexed item:\n",
    "\n",
    "1. the raw string `itemId` from Kaggle,\n",
    "2. the internal integer `iid`,\n",
    "3. the book `title`,\n",
    "4. a single review text field chosen from `review/summary` or `review/text` (whichever is available).\n",
    "\n",
    "The result is saved as `csv_export/amazonbooks_metadata_with_title_desc.csv` and provides a compact description per item suitable for quick inspection or simple LLM prompts."
   ],
   "id": "b39399e9aee3d82d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:10:50.213135Z",
     "start_time": "2025-12-06T18:10:11.465871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:/Users/carlk/OneDrive/Documents/uoft/ECE1508H F/Project\")\n",
    "\n",
    "item_map = pd.read_parquet(BASE / \"splits\" / \"item_id_map.parquet\")\n",
    "print(\"item_map:\", item_map.shape)\n",
    "\n",
    "meta_path = BASE / \"Books_rating.csv\"\n",
    "raw = pd.read_csv(meta_path, dtype=str)\n",
    "print(\"raw metadata:\", raw.shape)\n",
    "\n",
    "description_col = \"review/summary\" if \"review/summary\" in raw.columns else \"review/text\"\n",
    "\n",
    "metadata = raw[[\"Id\", \"Title\", description_col]].copy()\n",
    "metadata = metadata.rename(columns={\n",
    "    \"Id\": \"itemId\",\n",
    "    \"Title\": \"title\",\n",
    "    description_col: \"description\"\n",
    "})\n",
    "print(\"metadata cleaned:\", metadata.shape)\n",
    "\n",
    "merged = item_map.merge(metadata, on=\"itemId\", how=\"left\")\n",
    "print(\"Final merged shape:\", merged.shape)\n",
    "\n",
    "out_file = BASE / \"csv_export\" / \"amazonbooks_metadata_with_title_desc.csv\"\n",
    "merged.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"\\nSaved final metadata to:\", out_file)"
   ],
   "id": "5107164341df49c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_map: (18783, 2)\n",
      "raw metadata: (3000000, 10)\n",
      "metadata cleaned: (3000000, 3)\n",
      "Final merged shape: (1274120, 4)\n",
      "\n",
      "Saved final metadata to: C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\csv_export\\amazonbooks_metadata_with_title_desc.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8.1 Quick quality checks on metadata\n",
    "\n",
    "I load `amazonbooks_metadata_with_title_desc.csv` and verify that:\n",
    "\n",
    "1. All required columns (`itemId`, `iid`, `title`, `description`) are present.\n",
    "2. `itemId` and `iid` are unique and aligned (no duplicated IDs).\n",
    "3. Missing titles or descriptions are rare or absent.\n",
    "4. Sample rows look reasonable and match expectations for well-formed book metadata.\n",
    "\n",
    "These checks ensure the basic metadata table is clean before building richer descriptions."
   ],
   "id": "da80832b4aa053db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:10:58.919965Z",
     "start_time": "2025-12-06T18:10:56.292270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:/Users/carlk/OneDrive/Documents/uoft/ECE1508H F/Project\")\n",
    "meta_file = BASE / \"csv_export\" / \"amazonbooks_metadata_with_title_desc.csv\"\n",
    "\n",
    "print(\"Loading:\", meta_file)\n",
    "df = pd.read_csv(meta_file, dtype=str)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "print(\"\\n[1] Check required columns\")\n",
    "for col in [\"itemId\", \"iid\", \"title\", \"description\"]:\n",
    "    print(f\"  {col}: {'OK' if col in df.columns else 'MISSING'}\")\n",
    "\n",
    "print(\"\\n[2] Uniqueness checks\")\n",
    "print(\"  unique itemId:\", df[\"itemId\"].nunique())\n",
    "print(\"  rows (should match if no duplicated itemId):\", len(df))\n",
    "print(\"  unique iid    :\", df[\"iid\"].nunique())\n",
    "\n",
    "print(\"\\n[3] Missing values\")\n",
    "print(\"  missing title       :\", df[\"title\"].isna().sum())\n",
    "print(\"  missing description :\", df[\"description\"].isna().sum())\n",
    "\n",
    "print(\"\\n[4] Sample rows\")\n",
    "print(df.head(10))"
   ],
   "id": "4fbc79548d046263",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\csv_export\\amazonbooks_metadata_with_title_desc.csv\n",
      "Loaded shape: (1274120, 4)\n",
      "Columns: ['itemId', 'iid', 'title', 'description']\n",
      "\n",
      "[1] Check required columns\n",
      "  itemId: OK\n",
      "  iid: OK\n",
      "  title: OK\n",
      "  description: OK\n",
      "\n",
      "[2] Uniqueness checks\n",
      "  unique itemId: 18783\n",
      "  rows (should match if no duplicated itemId): 1274120\n",
      "  unique iid    : 18783\n",
      "\n",
      "[3] Missing values\n",
      "  missing title       : 0\n",
      "  missing description : 147\n",
      "\n",
      "[4] Sample rows\n",
      "       itemId iid                  title  \\\n",
      "0  0001047655   0  The Prodigal Daughter   \n",
      "1  0001047655   0  The Prodigal Daughter   \n",
      "2  0001047655   0  The Prodigal Daughter   \n",
      "3  0001047655   0  The Prodigal Daughter   \n",
      "4  0001047655   0  The Prodigal Daughter   \n",
      "5  0001047655   0  The Prodigal Daughter   \n",
      "6  0001047655   0  The Prodigal Daughter   \n",
      "7  0001047655   0  The Prodigal Daughter   \n",
      "8  0001047655   0  The Prodigal Daughter   \n",
      "9  0001047655   0  The Prodigal Daughter   \n",
      "\n",
      "                               description  \n",
      "0                               Who Cares?  \n",
      "1                LA OBRA MAESTRA DE ARCHER  \n",
      "2                  Extremly desapointing!!  \n",
      "3                             A great read  \n",
      "4                    The Prodigal Daughter  \n",
      "5  The Prodigal Daughter by Jeffrey Archer  \n",
      "6                          Worth your time  \n",
      "7                         Favorite author.  \n",
      "8                    The Prodical Daughter  \n",
      "9                      A well written book  \n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8.2 Inspect raw Kaggle review columns\n",
    "\n",
    "I briefly inspect the original `Books_rating.csv` schema to understand which text fields are available (`review/summary`, `review/text`, etc.).\n",
    "This informs how to combine multiple reviews into a single, cleaned description per item in the next step."
   ],
   "id": "8d324a21d8b27ad6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:12:02.439541Z",
     "start_time": "2025-12-06T18:11:04.877890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw = pd.read_csv(\"C:/Users/carlk/OneDrive/Documents/uoft/ECE1508H F/Project/Books_rating.csv\")\n",
    "print(raw.columns)"
   ],
   "id": "af696495b931046c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness',\n",
      "       'review/score', 'review/time', 'review/summary', 'review/text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Merge multiple reviews into short per-item descriptions\n",
    "\n",
    "To create richer, LLM-friendly metadata, I aggregate multiple reviews per item into a single cleaned description:\n",
    "\n",
    "1. Read the raw review text for each `(itemId, Title)` pair from the original CSV.\n",
    "2. Join with `item_id_map` so every review row has an integer `iid`.\n",
    "3. For each `(itemId, iid, title)` group, select up to 5 informative reviews.\n",
    "4. Concatenate them, decode basic HTML entities, and truncate the description to at most 400 characters, cutting at word boundaries when possible.\n",
    "\n",
    "The final per-item table `(itemId, iid, title, description)` is saved as `metadata/amazonbooks_metadata_merged_per_iid_clean.csv`."
   ],
   "id": "1c84da050237a97b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:13:03.999743Z",
     "start_time": "2025-12-06T18:12:29.946613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from html import unescape\n",
    "\n",
    "# Paths\n",
    "RAW_CSV_PATH = BASE / \"Books_rating.csv\"              # original Kaggle CSV\n",
    "ITEM_MAP_PATH = BASE / \"splits\" / \"item_id_map.parquet\"\n",
    "OUT_PATH      = BASE / \"metadata\" / \"amazonbooks_metadata_merged_per_iid_clean.csv\"\n",
    "\n",
    "assert RAW_CSV_PATH.exists(),  f\"Missing raw CSV: {RAW_CSV_PATH}\"\n",
    "assert ITEM_MAP_PATH.exists(), f\"Missing item_id_map: {ITEM_MAP_PATH}\"\n",
    "\n",
    "# Load raw reviews (itemId, title, review text)\n",
    "raw = pd.read_csv(\n",
    "    RAW_CSV_PATH,\n",
    "    usecols=[\"Id\", \"Title\", \"review/text\"],\n",
    ")\n",
    "raw = raw.rename(columns={\"Id\": \"itemId\", \"Title\": \"title\", \"review/text\": \"review\"})\n",
    "raw = raw.dropna(subset=[\"title\"])\n",
    "\n",
    "print(\"Raw reviews rows:\", len(raw))\n",
    "\n",
    "# Join with item_id_map so every row has an iid\n",
    "item_map = pd.read_parquet(ITEM_MAP_PATH)   # columns: itemId, iid\n",
    "df = raw.merge(item_map, on=\"itemId\", how=\"inner\")\n",
    "print(\"After join with item_id_map, rows:\", len(df))\n",
    "print(\"Unique iids in joined reviews:\", df[\"iid\"].nunique())\n",
    "\n",
    "\n",
    "def build_description(group: pd.DataFrame,\n",
    "                      max_reviews: int = 5,\n",
    "                      max_chars: int = 400) -> str:\n",
    "    \"\"\"Merge up to max_reviews reviews into a short cleaned description.\"\"\"\n",
    "    texts = group[\"review\"].dropna().astype(str).head(max_reviews).tolist()\n",
    "    if not texts:\n",
    "        return \"\"\n",
    "\n",
    "    text = \" \".join(texts)\n",
    "    text = unescape(text)\n",
    "\n",
    "    if len(text) > max_chars:\n",
    "        cut = text.rfind(\" \", 0, max_chars)\n",
    "        if cut == -1:\n",
    "            cut = max_chars\n",
    "        text = text[:cut]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Build one description per iid (and title)\n",
    "meta = (\n",
    "    df.groupby([\"itemId\", \"iid\", \"title\"], as_index=False)\n",
    "      .apply(lambda g: pd.Series({\"description\": build_description(g)}))\n",
    ")\n",
    "\n",
    "print(\"Metadata rows (one per iid):\", len(meta))\n",
    "print(\"Unique titles in metadata:\", meta[\"title\"].nunique())\n",
    "\n",
    "# Save final metadata table\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "meta.to_csv(OUT_PATH, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved merged metadata to {OUT_PATH}\")"
   ],
   "id": "13778cbd33f094bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw reviews rows: 2999792\n",
      "After join with item_id_map, rows: 1274120\n",
      "Unique iids in joined reviews: 18783\n",
      "Metadata rows (one per iid): 18783\n",
      "Unique titles in metadata: 18783\n",
      "Saved merged metadata to C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\metadata\\amazonbooks_metadata_merged_per_iid_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlk\\AppData\\Local\\Temp\\ipykernel_14648\\2087733464.py:55: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: pd.Series({\"description\": build_description(g)}))\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9.1 Final validation of per-item metadata\n",
    "\n",
    "Finally, I check that the merged metadata file `amazonbooks_metadata_merged_per_iid_clean.csv` satisfies:\n",
    "\n",
    "1. Each `iid` maps to exactly one `title`.\n",
    "2. Each `title` maps to exactly one `iid`.\n",
    "3. The number of rows equals both the number of unique `iid` values and the number of unique titles.\n",
    "\n",
    "Passing these checks confirms that the cleaned metadata has a strict one-to-one mapping between internal item indices and human-readable book titles, making it safe for downstream models and LLM prompts."
   ],
   "id": "638ef33a978214bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T18:13:36.451997Z",
     "start_time": "2025-12-06T18:13:36.175577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the final per-iid metadata file\n",
    "META_PATH = BASE / \"metadata\" / \"amazonbooks_metadata_merged_per_iid_clean.csv\"\n",
    "\n",
    "assert META_PATH.exists(), f\"Missing metadata file: {META_PATH}\"\n",
    "\n",
    "df = pd.read_csv(META_PATH)\n",
    "\n",
    "print(\"Loaded metadata:\", META_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# Basic uniqueness counts\n",
    "print(\"\\n[1] Basic uniqueness stats\")\n",
    "n_iid = df[\"iid\"].nunique()\n",
    "n_title = df[\"title\"].nunique()\n",
    "n_rows = len(df)\n",
    "\n",
    "print(\" unique iid   :\", n_iid)\n",
    "print(\" unique title :\", n_title)\n",
    "print(\" rows         :\", n_rows)\n",
    "\n",
    "# Check that each iid has exactly one title\n",
    "print(\"\\n[2] Check: at most one title per iid\")\n",
    "titles_per_iid = df.groupby(\"iid\")[\"title\"].nunique()\n",
    "max_titles_per_iid = titles_per_iid.max()\n",
    "iids_with_multiple_titles = (titles_per_iid > 1).sum()\n",
    "\n",
    "print(\" max titles per iid        :\", max_titles_per_iid)\n",
    "print(\" iids with >1 different title:\", iids_with_multiple_titles)\n",
    "\n",
    "# Check that each title maps to exactly one iid\n",
    "print(\"\\n[3] Check: at most one iid per title\")\n",
    "iids_per_title = df.groupby(\"title\")[\"iid\"].nunique()\n",
    "max_iids_per_title = iids_per_title.max()\n",
    "titles_with_multiple_iids = (iids_per_title > 1).sum()\n",
    "\n",
    "print(\" max iids per title        :\", max_iids_per_title)\n",
    "print(\" titles with >1 different iid:\", titles_with_multiple_iids)\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n[4] Summary\")\n",
    "if max_titles_per_iid == 1 and max_iids_per_title == 1:\n",
    "    print(\"OK: Each iid and each title form a one-to-one mapping (titles are unique after processing).\")\n",
    "else:\n",
    "    print(\"WARNING: There are still iid<->title conflicts. Please inspect the counts above.\")"
   ],
   "id": "b9f94583b93400e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata: C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\metadata\\amazonbooks_metadata_merged_per_iid_clean.csv\n",
      "Shape: (18783, 4)\n",
      "Columns: ['itemId', 'iid', 'title', 'description']\n",
      "\n",
      "[1] Basic uniqueness stats\n",
      " unique iid   : 18783\n",
      " unique title : 18783\n",
      " rows         : 18783\n",
      "\n",
      "[2] Check: at most one title per iid\n",
      " max titles per iid        : 1\n",
      " iids with >1 different title: 0\n",
      "\n",
      "[3] Check: at most one iid per title\n",
      " max iids per title        : 1\n",
      " titles with >1 different iid: 0\n",
      "\n",
      "[4] Summary\n",
      "OK: Each iid and each title form a one-to-one mapping (titles are unique after processing).\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
